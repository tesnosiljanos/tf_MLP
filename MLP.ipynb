{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy      as np\n",
    "\n",
    "'''class MLP which contains all methods needed to train and test the performance of your model'''\n",
    "class MLP:\n",
    "    \n",
    "    '''initializer function which holds all the hyperparameters''' \n",
    "    def __init__(self, \n",
    "                 input_size      = 100, # sepcific to the size of input vector\n",
    "                 output_size     = 2,   # specific to the size of output vector  \n",
    "                 mini_batch_size = 100, # learning is done via mini batches, this specifies their size. = 1 is SGD\n",
    "                 layers          = [10, 10], # the hidden layer structure of the MLP, a list where len(list) = # of hidden layers\n",
    "                 l_r             = 0.0001, # starting learning rate \n",
    "                 kp              = 0.95,  # keep probability of dropout regularizer, 1 = no dropout applied\n",
    "                 l1              = 0.001, # constant multiplier applied to L1 regularizer, l1 = 0 = no L1 regularization\n",
    "                 l2              = 0.001, # constant multiplier applied to L2 regularizer, l2 = 0 = no L2 regularization\n",
    "                 activation      = 'sigmoid'): # activation function possible inputs are 'sigmoid','relu','tanh'\n",
    "        \n",
    "        \n",
    "        # setting all the input variables as global \n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.kp = kp\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.l_r = l_r\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        # initializing lists which will hold network weights biases z's and layer activations\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.z = []\n",
    "        self.a = []\n",
    "        # initializing list which will hold the errors as learning progresses\n",
    "        self.train_memory = []\n",
    "        self.valid_memory = []\n",
    "        self.test_memory  = []\n",
    "\n",
    "    '''here we define the tensorflow model function, a bunch of mathematical operations to be performed on the input'''   \n",
    "    def model(self): \n",
    "        \n",
    "        # these are pllaceholders, a way of telling tensorflow that he is to expect some data of this shape\n",
    "        self.inputs = tf.placeholder(shape = [None, self.input_size] , dtype = 'float32')\n",
    "        self.label  = tf.placeholder(shape = [None, self.output_size], dtype = 'float32')\n",
    "        self.lr     = tf.placeholder(tf.float32, shape = ())\n",
    "        \n",
    "        \n",
    "        self.a.append(self.inputs)\n",
    "        \n",
    "        in_size  = self.inputs.shape[1]\n",
    "        out_size = self.label.shape[1]\n",
    "        \n",
    "        # here we calculate everything about the hidden layers\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            self.W.append(tf.Variable(tf.zeros(shape = [in_size, self.layers[i]], dtype = 'float32')))\n",
    "            self.B.append(tf.Variable(tf.zeros(shape = [self.layers[i]],          dtype = 'float32')))\n",
    "\n",
    "            self.z.append(tf.add(tf.matmul(self.a[i], self.W[i]), self.B[i]))\n",
    "\n",
    "            if self.activation == 'relu': \n",
    "                self.a.append(tf.nn.dropout(tf.nn.relu(self.z[i]),keep_prob = self.kp))\n",
    "            elif self.activation == 'sigmoid':\n",
    "                self.a.append(tf.nn.dropout(tf.sigmoid(self.z[i]),keep_prob = self.kp))\n",
    "            elif self.activation == 'tanh':\n",
    "                self.a.append(tf.nn.dropout(tf.tanh(self.z[i]),   keep_prob = self.kp))\n",
    "                \n",
    "            in_size = self.layers[i]\n",
    "\n",
    "        # here we calculate everything about the output layer\n",
    "        self.W.append(tf.Variable(tf.zeros(shape = [in_size, out_size], dtype = 'float32')))\n",
    "        self.B.append(tf.Variable(tf.zeros(shape = [out_size],          dtype = 'float32')))\n",
    "\n",
    "        self.z.append(tf.add(tf.matmul(self.a[-1], self.W[-1]), self.B[-1]))\n",
    "        self.a.append(self.z[-1])\n",
    "        \n",
    "        # final output prediction\n",
    "        self.yo = self.a[-1]\n",
    "\n",
    "        # here we flatten the weights to prepare for calculation for regularizers\n",
    "        weights = []\n",
    "        \n",
    "        for i, w in enumerate(self.W):\n",
    "            weights += [tf.reshape(tensor = w, shape = [-1])]\n",
    "            \n",
    "        weights = tf.concat(weights, axis = 0)           \n",
    "        \n",
    "        \n",
    "        # here we define our loss function, we have two since we declare MSE error, but train on regularized loss\n",
    "        self.mse       = tf.reduce_mean(tf.squared_difference(self.yo,self.label))\n",
    "        self.loss      = self.mse + self.l2 * tf.reduce_mean(tf.square(weights)) + self.l1 * tf.reduce_mean(tf.abs(weights))\n",
    "        \n",
    "        # here we specify out optimizer and the loss function which we want to minimise\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # here we initialize the tensorflow global variables\n",
    "        self.init  = tf.global_variables_initializer()\n",
    "        \n",
    "    '''A function which creates a subsample of input data'''\n",
    "    def minibatch(self, X_batch, Y_batch):\n",
    "        \n",
    "        mask = np.random.choice(len(X_batch), self.mini_batch_size)\n",
    "        \n",
    "        mini_batch_X = X_batch[mask]\n",
    "        mini_batch_Y = Y_batch[mask]\n",
    "        \n",
    "        return mini_batch_X, mini_batch_Y\n",
    "    \n",
    "    \n",
    "    '''A function which determines the mean squared error of the prediction vs ground truth'''\n",
    "    def evaluate(self, X, Y):\n",
    "        return self.sess.run(self.mse , feed_dict={self.inputs : X, self.label  : Y})\n",
    "    \n",
    "    '''The train function which takes in the training data and trains the model, \n",
    "       it also calculates the error on validation and test data'''\n",
    "    def train(self, X_train, Y_train, X_valid, Y_valid, X_test, Y_test, epochs = 1):\n",
    "        \n",
    "        total_batch = int(len(X_batch)/self.mini_batch_size)\n",
    "        \n",
    "        for epoch in range(epochs): # epochs are how many times we go through the whole dataset\n",
    "            \n",
    "            print('Epoch: {:3d}    Learning Rate: {:.10f}'.format(epoch, self.l_r/(10*(epoch+1))))\n",
    "            \n",
    "            for i in range(total_batch): # inside one epoch we perform as many minibatches as you can fit into the whole dataset \n",
    "                \n",
    "                mini_X_batch, mini_Y_batch = self.minibatch(X_train, Y_train)\n",
    "              \n",
    "                avg_cost = 0.0\n",
    "                \n",
    "                # the tensorflow optimisation is run, also mean squared error is print out. learning rate is annealing\n",
    "                _, c = self.sess.run([self.optimizer, self.mse],  feed_dict = {self.lr     : self.l_r/(10*(epoch+1)), \n",
    "                                                                               self.inputs : mini_X_batch,\n",
    "                                                                               self.label  : mini_Y_batch})\n",
    "                avg_cost += c\n",
    "            \n",
    "            avg_cost /= total_batch    \n",
    "\n",
    "            if self.verbose == True: # if you want to see progress of each epoch you can\n",
    "                print(\"Epoch: {:3d}    Train MSE: {:.8f}\".format(epoch, avg_cost))\n",
    "            \n",
    "            #after each training epoch have elapsed we perform an evaluation on validation and test datasets\n",
    "            validation_mse = self.evaluate(X_valid, Y_valid)\n",
    "            test_mse       = self.evaluate(X_test,  Y_test)\n",
    "            \n",
    "            # we append the results from the training epoch to a list, \n",
    "            # so we can plot a graph of error w.r.t. learning fate for example\n",
    "            self.train_memory.append(avg_cost)\n",
    "            self.valid_memory.append(validation_mse)\n",
    "            self.test_memory.append(test_mse)\n",
    "        \n",
    "            # i break the epoch loop if the training error is really small, you might want to comment it out\n",
    "            if avg_cost < 1e-7:\n",
    "                break\n",
    "                \n",
    "        return avg_cost\n",
    "\n",
    "\n",
    "    '''a function where all the other functions are called'''\n",
    "    def learn(self, \n",
    "              X_train, Y_train, \n",
    "              X_valid, Y_valid, \n",
    "              X_test, Y_test, \n",
    "              epochs = 1, verbose = False):\n",
    "        \n",
    "        # we make sure tensorflow forgets everything\n",
    "        tf.reset_default_graph()\n",
    "        self.verbose = verbose\n",
    "        # calling the model fucntion to initialize model and variables\n",
    "        self.model()\n",
    "        # create and open a tensorflow session\n",
    "        self.sess = tf.Session()\n",
    "        # run the tensorflow global variables initializer \n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "        \n",
    "        train_mse      = self.train(X_train, Y_train, X_valid, Y_valid, X_test, Y_test, epochs) \n",
    "        validation_mse = self.evaluate(X_valid, Y_valid)\n",
    "        test_mse       = self.evaluate(X_test,  Y_test)\n",
    "        \n",
    "        self.sess.close()\n",
    "        \n",
    "        return train_mse, validation_mse, test_mse\n",
    "    \n",
    "\n",
    "'''To execute code above using the methods you might want to do something like this'''\n",
    "'''We create the class and initialize main parameters'''\n",
    "mpl = MLP(input_size     = 100,\n",
    "          output_size    = 5,\n",
    "          layers         = [10,10,10], \n",
    "          l_r            = 0.000001, \n",
    "          activation     = 'sigmoid', \n",
    "          mini_batch_size= 50, \n",
    "          kp             = 0.9, \n",
    "          l1             = 1, \n",
    "          l2             = 1)\n",
    "'''We call the learn method of mlp class which returns three mean squared errors'''\n",
    "train_error, validation_error, test_error = mpl.learn(my_X_train     , my_Y_train,                 \n",
    "                                                      my_X_validation, my_Y_validation,\n",
    "                                                      my_X_test      , my_Y_test,\n",
    "                                                      epochs = 50, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
